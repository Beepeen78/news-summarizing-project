{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e920623a",
   "metadata": {},
   "source": [
    "Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6c8240d-fbab-4a56-8e98-188e02f6d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_from_disk\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf98abe",
   "metadata": {},
   "source": [
    "Loading the dataset & data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('validation.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41ec61cc-93d6-4bac-8e72-07a5f629ad24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         id  \\\n",
      "0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
      "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
      "2  00027e965c8264c35cc1bc55556db388da82b07f   \n",
      "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
      "4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n",
      "\n",
      "                                             article  \\\n",
      "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
      "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
      "2  A drunk driver who killed a young woman in a h...   \n",
      "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
      "4  Fleetwood are the only team still to have a 10...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Bishop John Folda, of North Dakota, is taking ...  \n",
      "1  Criminal complaint: Cop used his role to help ...  \n",
      "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
      "3  Nina dos Santos says Europe must be ready to a...  \n",
      "4  Fleetwood top of League One after 2-0 win at S...  \n",
      "                                         id  \\\n",
      "0  61df4979ac5fcc2b71be46ed6fe5a46ce7f071c3   \n",
      "1  21c0bd69b7e7df285c3d1b1cf56d4da925980a68   \n",
      "2  56f340189cd128194b2e7cb8c26bb900e3a848b4   \n",
      "3  00a665151b89a53e5a08a389df8334f4106494c2   \n",
      "4  9f6fbd3c497c4d28879bebebea220884f03eb41a   \n",
      "\n",
      "                                             article  \\\n",
      "0  Sally Forrest, an actress-dancer who graced th...   \n",
      "1  A middle-school teacher in China has inked hun...   \n",
      "2  A man convicted of killing the father and sist...   \n",
      "3  Avid rugby fan Prince Harry could barely watch...   \n",
      "4  A Triple M Radio producer has been inundated w...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Sally Forrest, an actress-dancer who graced th...  \n",
      "1  Works include pictures of Presidential Palace ...  \n",
      "2  Iftekhar Murtaza, 29, was convicted a year ago...  \n",
      "3  Prince Harry in attendance for England's crunc...  \n",
      "4  Nick Slater's colleagues uploaded a picture to...  \n",
      "                                         id  \\\n",
      "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
      "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
      "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
      "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
      "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
      "\n",
      "                                             article  \\\n",
      "0  Ever noticed how plane seats appear to be gett...   \n",
      "1  A drunk teenage boy had to be rescued by secur...   \n",
      "2  Dougie Freedman is on the verge of agreeing a ...   \n",
      "3  Liverpool target Neto is also wanted by PSG an...   \n",
      "4  Bruce Jenner will break his silence in a two-h...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Experts question if  packed out planes are put...  \n",
      "1  Drunk teenage boy climbed into lion enclosure ...  \n",
      "2  Nottingham Forest are close to extending Dougi...  \n",
      "3  Fiorentina goalkeeper Neto has been linked wit...  \n",
      "4  Tell-all interview with the reality TV star, 6...  \n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())\n",
    "print(val_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554f2fef",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76967017-c4cc-4f9f-992c-578527fd25ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing all letters to lowercase\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "train_data['article'] = train_data['article'].apply(preprocess)\n",
    "train_data['highlights'] = train_data['highlights'].apply(preprocess)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(preprocess)\n",
    "val_data['highlights'] = val_data['highlights'].apply(preprocess)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(preprocess)\n",
    "test_data['highlights'] = test_data['highlights'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a95d2-2009-4c61-8961-84edecf949b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to Hugging Face Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09ee66-310a-422b-8fac-0381a78f50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned datasets as CSV files without including the index\n",
    "train_data.to_csv('cleaned_train_data.csv', index=False)\n",
    "val_data.to_csv('cleaned_val_data.csv', index=False)\n",
    "test_data.to_csv('cleaned_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472fbe6-bcab-4df6-ae5e-04918aac472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK resources: stopwords list and punkt tokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Create a set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to remove stopwords from a given text\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)  # Tokenize the text into words\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]  # Filter out stopwords\n",
    "    return ' '.join(filtered_text)  # Join the words back into a single string\n",
    "\n",
    "# Apply the remove_stopwords function to the 'article' and 'highlights' columns of the datasets\n",
    "train_data['article'] = train_data['article'].apply(remove_stopwords)\n",
    "train_data['highlights'] = train_data['highlights'].apply(remove_stopwords)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(remove_stopwords)\n",
    "val_data['highlights'] = val_data['highlights'].apply(remove_stopwords)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(remove_stopwords)\n",
    "test_data['highlights'] = test_data['highlights'].apply(remove_stopwords)\n",
    "\n",
    "# Save the datasets after removing stopwords\n",
    "train_data.to_csv('stopwords_removed_train_data.csv', index=False)\n",
    "val_data.to_csv('stopwords_removed_val_data.csv', index=False)\n",
    "test_data.to_csv('stopwords_removed_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7572f-584f-46e2-9444-213909e457ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use SpaCy for lemmatization to convert words to their base forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b4c37b2-8247-4c47-a030-9b20f7ef8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c9e9eb-6986-4be5-a487-089c9c91a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy tqdm\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48cd0ac3-f788-4657-9771-3bc72a822942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 287113/287113 [3:35:38<00:00, 22.19it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 287113/287113 [14:04<00:00, 339.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 13368/13368 [08:01<00:00, 27.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 13368/13368 [00:41<00:00, 318.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 11490/11490 [07:00<00:00, 27.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 11490/11490 [00:35<00:00, 326.70it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('cleaned_train_data.csv')\n",
    "val_data = pd.read_csv('cleaned_val_data.csv')\n",
    "test_data = pd.read_csv('cleaned_test_data.csv')\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to all datasets\n",
    "train_data['article'] = train_data['article'].apply(clean_text)\n",
    "train_data['highlights'] = train_data['highlights'].apply(clean_text)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(clean_text)\n",
    "val_data['highlights'] = val_data['highlights'].apply(clean_text)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(clean_text)\n",
    "test_data['highlights'] = test_data['highlights'].apply(clean_text)\n",
    "\n",
    "# Load the pre-trained SpaCy model globally\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function for lemmatization using nlp.pipe for batch processing\n",
    "def lemmatize_texts_with_progress(texts):\n",
    "    lemmatized_texts = []\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=50, disable=['parser', 'ner']), total=len(texts)):\n",
    "        lemmatized_texts.append(' '.join([token.lemma_ for token in doc]))\n",
    "    return lemmatized_texts\n",
    "\n",
    "# Apply lemmatization with progress bar\n",
    "train_data['article'] = lemmatize_texts_with_progress(train_data['article'].tolist())\n",
    "train_data['highlights'] = lemmatize_texts_with_progress(train_data['highlights'].tolist())\n",
    "\n",
    "val_data['article'] = lemmatize_texts_with_progress(val_data['article'].tolist())\n",
    "val_data['highlights'] = lemmatize_texts_with_progress(val_data['highlights'].tolist())\n",
    "\n",
    "test_data['article'] = lemmatize_texts_with_progress(test_data['article'].tolist())\n",
    "test_data['highlights'] = lemmatize_texts_with_progress(test_data['highlights'].tolist())\n",
    "\n",
    "# Save lemmatized data\n",
    "train_data.to_csv('lemmatized_train_data.csv', index=False)\n",
    "val_data.to_csv('lemmatized_val_data.csv', index=False)\n",
    "test_data.to_csv('lemmatized_test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63fecb34-a6f3-4df0-8b0a-361bb08c8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Function to replace all numbers in the text with a placeholder <NUM>\n",
    "def replace_numbers(text):\n",
    "    return re.sub(r'\\d+', '<NUM>', text)\n",
    "\n",
    "# Function to replace rare words in the text with a placeholder <RARE>\n",
    "# A word is considered rare if its frequency is below the specified threshold (default is 5)\n",
    "def remove_rare_words(text, freq_threshold=5):\n",
    "    words = text.split()  # Split the text into words\n",
    "    word_freq = Counter(words)  # Count the frequency of each word\n",
    "    # Identify words that occur less than the frequency threshold\n",
    "    rare_words = {word for word, freq in word_freq.items() if freq < freq_threshold}\n",
    "    # Replace rare words with <RARE>, keep other words unchanged\n",
    "    filtered_text = [word if word not in rare_words else '<RARE>' for word in words]\n",
    "    return ' '.join(filtered_text)  # Join the words back into a single string\n",
    "\n",
    "# Apply number replacement and rare word removal to the 'article' and 'highlights' columns of all datasets\n",
    "train_data['article'] = train_data['article'].apply(replace_numbers).apply(remove_rare_words)\n",
    "train_data['highlights'] = train_data['highlights'].apply(replace_numbers).apply(remove_rare_words)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(replace_numbers).apply(remove_rare_words)\n",
    "val_data['highlights'] = val_data['highlights'].apply(replace_numbers).apply(remove_rare_words)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(replace_numbers).apply(remove_rare_words)\n",
    "test_data['highlights'] = test_data['highlights'].apply(replace_numbers).apply(remove_rare_words)\n",
    "\n",
    "# Save the processed datasets with handled numbers and rare words\n",
    "train_data.to_csv('handled_train_data.csv', index=False)  # Save the training data\n",
    "val_data.to_csv('handled_val_data.csv', index=False)      # Save the validation data\n",
    "test_data.to_csv('handled_test_data.csv', index=False)    # Save the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bbd06f1-63c3-4a2b-a698-c0a26066fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentence tokenization to the 'article' column\n",
    "# This splits the text of each article into a list of sentences\n",
    "train_data['sentences'] = train_data['article'].apply(sent_tokenize)\n",
    "val_data['sentences'] = val_data['article'].apply(sent_tokenize)\n",
    "test_data['sentences'] = test_data['article'].apply(sent_tokenize)\n",
    "\n",
    "# Save the datasets with the new 'sentences' column to CSV files\n",
    "train_data.to_csv('tokenized_train_data.csv', index=False)  # Save the training data\n",
    "val_data.to_csv('tokenized_val_data.csv', index=False)      # Save the validation data\n",
    "test_data.to_csv('tokenized_test_data.csv', index=False)    # Save the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7862220-f2cc-47d8-9e41-09722831bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to Hugging Face Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
