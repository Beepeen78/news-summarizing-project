{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56117190-3e48-4cc1-b0ab-f6a0ba22817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d662b4d8-2cd0-4362-9872-6af97acfd7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('validation.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6c8240d-fbab-4a56-8e98-188e02f6d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_from_disk\n",
    "import accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41ec61cc-93d6-4bac-8e72-07a5f629ad24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         id  \\\n",
      "0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
      "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
      "2  00027e965c8264c35cc1bc55556db388da82b07f   \n",
      "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
      "4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n",
      "\n",
      "                                             article  \\\n",
      "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
      "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
      "2  A drunk driver who killed a young woman in a h...   \n",
      "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
      "4  Fleetwood are the only team still to have a 10...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Bishop John Folda, of North Dakota, is taking ...  \n",
      "1  Criminal complaint: Cop used his role to help ...  \n",
      "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
      "3  Nina dos Santos says Europe must be ready to a...  \n",
      "4  Fleetwood top of League One after 2-0 win at S...  \n",
      "                                         id  \\\n",
      "0  61df4979ac5fcc2b71be46ed6fe5a46ce7f071c3   \n",
      "1  21c0bd69b7e7df285c3d1b1cf56d4da925980a68   \n",
      "2  56f340189cd128194b2e7cb8c26bb900e3a848b4   \n",
      "3  00a665151b89a53e5a08a389df8334f4106494c2   \n",
      "4  9f6fbd3c497c4d28879bebebea220884f03eb41a   \n",
      "\n",
      "                                             article  \\\n",
      "0  Sally Forrest, an actress-dancer who graced th...   \n",
      "1  A middle-school teacher in China has inked hun...   \n",
      "2  A man convicted of killing the father and sist...   \n",
      "3  Avid rugby fan Prince Harry could barely watch...   \n",
      "4  A Triple M Radio producer has been inundated w...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Sally Forrest, an actress-dancer who graced th...  \n",
      "1  Works include pictures of Presidential Palace ...  \n",
      "2  Iftekhar Murtaza, 29, was convicted a year ago...  \n",
      "3  Prince Harry in attendance for England's crunc...  \n",
      "4  Nick Slater's colleagues uploaded a picture to...  \n",
      "                                         id  \\\n",
      "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
      "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
      "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
      "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
      "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
      "\n",
      "                                             article  \\\n",
      "0  Ever noticed how plane seats appear to be gett...   \n",
      "1  A drunk teenage boy had to be rescued by secur...   \n",
      "2  Dougie Freedman is on the verge of agreeing a ...   \n",
      "3  Liverpool target Neto is also wanted by PSG an...   \n",
      "4  Bruce Jenner will break his silence in a two-h...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Experts question if  packed out planes are put...  \n",
      "1  Drunk teenage boy climbed into lion enclosure ...  \n",
      "2  Nottingham Forest are close to extending Dougi...  \n",
      "3  Fiorentina goalkeeper Neto has been linked wit...  \n",
      "4  Tell-all interview with the reality TV star, 6...  \n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())\n",
    "print(val_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76967017-c4cc-4f9f-992c-578527fd25ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "train_data['article'] = train_data['article'].apply(preprocess)\n",
    "train_data['highlights'] = train_data['highlights'].apply(preprocess)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(preprocess)\n",
    "val_data['highlights'] = val_data['highlights'].apply(preprocess)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(preprocess)\n",
    "test_data['highlights'] = test_data['highlights'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a95d2-2009-4c61-8961-84edecf949b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09ee66-310a-422b-8fac-0381a78f50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('cleaned_train_data.csv', index=False)\n",
    "val_data.to_csv('cleaned_val_data.csv', index=False)\n",
    "test_data.to_csv('cleaned_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a7a9e-d22c-48af-aeb6-274f8d02bc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Remove common words that don't add much meaning (like \"the\", \"is\", \"in\", etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472fbe6-bcab-4df6-ae5e-04918aac472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "train_data['article'] = train_data['article'].apply(remove_stopwords)\n",
    "train_data['highlights'] = train_data['highlights'].apply(remove_stopwords)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(remove_stopwords)\n",
    "val_data['highlights'] = val_data['highlights'].apply(remove_stopwords)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(remove_stopwords)\n",
    "test_data['highlights'] = test_data['highlights'].apply(remove_stopwords)\n",
    "\n",
    "# Save data after removing stopwords\n",
    "train_data.to_csv('stopwords_removed_train_data.csv', index=False)\n",
    "val_data.to_csv('stopwords_removed_val_data.csv', index=False)\n",
    "test_data.to_csv('stopwords_removed_test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7572f-584f-46e2-9444-213909e457ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use SpaCy for lemmatization to convert words to their base forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b4c37b2-8247-4c47-a030-9b20f7ef8177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.5-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.5-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (1.24.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.18.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.0-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.7.5-cp311-cp311-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.1 MB 5.3 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.4/12.1 MB 5.3 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.2/12.1 MB 9.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.0/12.1 MB 16.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.0/12.1 MB 21.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.8/12.1 MB 32.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 59.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 54.4 MB/s eta 0:00:00\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Using cached langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.3/122.3 kB 7.0 MB/s eta 0:00:00\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "   ---------------------------------------- 0.0/479.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 479.7/479.7 kB 29.3 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.5-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 45.9 MB/s eta 0:00:00\n",
      "Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 2.1/6.6 MB 65.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.0/6.6 MB 63.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.9/6.6 MB 42.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 41.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 35.1 MB/s eta 0:00:00\n",
      "Using cached cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.0-cp311-cp311-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.6/152.6 kB 8.9 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.18.1 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 murmurhash-1.0.10 preshed-3.0.9 shellingham-1.5.4 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.12.3 wasabi-1.1.3 weasel-0.4.1\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.1/12.8 MB 825.8 kB/s eta 0:00:16\n",
      "      --------------------------------------- 0.2/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "     -- ------------------------------------- 0.9/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.2/12.8 MB 14.4 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.3/12.8 MB 25.0 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 33.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.5/12.8 MB 65.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 54.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.0)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14c9e9eb-6986-4be5-a487-089c9c91a78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\alexi\\anaconda3\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alexi\\anaconda3\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy) (1.24.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 435.7 kB/s eta 0:00:30\n",
      "     --------------------------------------- 0.1/12.8 MB 657.6 kB/s eta 0:00:20\n",
      "      --------------------------------------- 0.3/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 1.1/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.2/12.8 MB 13.4 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.4/12.8 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.9/12.8 MB 30.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.5/12.8 MB 65.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.5/12.8 MB 65.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.5/12.8 MB 65.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.5/12.8 MB 65.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.5/12.8 MB 65.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.5/12.8 MB 65.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 24.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 21.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.7/12.8 MB 19.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 18.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 17.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.0)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy tqdm\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48cd0ac3-f788-4657-9771-3bc72a822942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 287113/287113 [3:35:38<00:00, 22.19it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 287113/287113 [14:04<00:00, 339.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 13368/13368 [08:01<00:00, 27.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 13368/13368 [00:41<00:00, 318.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 11490/11490 [07:00<00:00, 27.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 11490/11490 [00:35<00:00, 326.70it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('cleaned_train_data.csv')\n",
    "val_data = pd.read_csv('cleaned_val_data.csv')\n",
    "test_data = pd.read_csv('cleaned_test_data.csv')\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to all datasets\n",
    "train_data['article'] = train_data['article'].apply(clean_text)\n",
    "train_data['highlights'] = train_data['highlights'].apply(clean_text)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(clean_text)\n",
    "val_data['highlights'] = val_data['highlights'].apply(clean_text)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(clean_text)\n",
    "test_data['highlights'] = test_data['highlights'].apply(clean_text)\n",
    "\n",
    "# Load the pre-trained SpaCy model globally\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function for lemmatization using nlp.pipe for batch processing\n",
    "def lemmatize_texts_with_progress(texts):\n",
    "    lemmatized_texts = []\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=50, disable=['parser', 'ner']), total=len(texts)):\n",
    "        lemmatized_texts.append(' '.join([token.lemma_ for token in doc]))\n",
    "    return lemmatized_texts\n",
    "\n",
    "# Apply lemmatization with progress bar\n",
    "train_data['article'] = lemmatize_texts_with_progress(train_data['article'].tolist())\n",
    "train_data['highlights'] = lemmatize_texts_with_progress(train_data['highlights'].tolist())\n",
    "\n",
    "val_data['article'] = lemmatize_texts_with_progress(val_data['article'].tolist())\n",
    "val_data['highlights'] = lemmatize_texts_with_progress(val_data['highlights'].tolist())\n",
    "\n",
    "test_data['article'] = lemmatize_texts_with_progress(test_data['article'].tolist())\n",
    "test_data['highlights'] = lemmatize_texts_with_progress(test_data['highlights'].tolist())\n",
    "\n",
    "# Save lemmatized data\n",
    "train_data.to_csv('lemmatized_train_data.csv', index=False)\n",
    "val_data.to_csv('lemmatized_val_data.csv', index=False)\n",
    "test_data.to_csv('lemmatized_test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63fecb34-a6f3-4df0-8b0a-361bb08c8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def replace_numbers(text):\n",
    "    return re.sub(r'\\d+', '<NUM>', text)\n",
    "\n",
    "def remove_rare_words(text, freq_threshold=5):\n",
    "    words = text.split()\n",
    "    word_freq = Counter(words)\n",
    "    rare_words = {word for word, freq in word_freq.items() if freq < freq_threshold}\n",
    "    filtered_text = [word if word not in rare_words else '<RARE>' for word in words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "train_data['article'] = train_data['article'].apply(replace_numbers).apply(remove_rare_words)\n",
    "train_data['highlights'] = train_data['highlights'].apply(replace_numbers).apply(remove_rare_words)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(replace_numbers).apply(remove_rare_words)\n",
    "val_data['highlights'] = val_data['highlights'].apply(replace_numbers).apply(remove_rare_words)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(replace_numbers).apply(remove_rare_words)\n",
    "test_data['highlights'] = test_data['highlights'].apply(replace_numbers).apply(remove_rare_words)\n",
    "\n",
    "# Save data after handling numbers and rare words\n",
    "train_data.to_csv('handled_train_data.csv', index=False)\n",
    "val_data.to_csv('handled_val_data.csv', index=False)\n",
    "test_data.to_csv('handled_test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6f2557d-46b3-4cb2-b6f1-d9907efa4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def replace_numbers(text):\n",
    "    return re.sub(r'\\d+', '<NUM>', text)\n",
    "\n",
    "def remove_rare_words(text, freq_threshold=5):\n",
    "    words = text.split()\n",
    "    word_freq = Counter(words)\n",
    "    rare_words = {word for word, freq in word_freq.items() if freq < freq_threshold}\n",
    "    filtered_text = [word if word not in rare_words else '<RARE>' for word in words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "train_data['article'] = train_data['article'].apply(replace_numbers).apply(remove_rare_words)\n",
    "train_data['highlights'] = train_data['highlights'].apply(replace_numbers).apply(remove_rare_words)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(replace_numbers).apply(remove_rare_words)\n",
    "val_data['highlights'] = val_data['highlights'].apply(replace_numbers).apply(remove_rare_words)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(replace_numbers).apply(remove_rare_words)\n",
    "test_data['highlights'] = test_data['highlights'].apply(replace_numbers).apply(remove_rare_words)\n",
    "\n",
    "# Save data after handling numbers and rare words\n",
    "train_data.to_csv('handled_train_data.csv', index=False)\n",
    "val_data.to_csv('handled_val_data.csv', index=False)\n",
    "test_data.to_csv('handled_test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bbd06f1-63c3-4a2b-a698-c0a26066fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "train_data['sentences'] = train_data['article'].apply(sent_tokenize)\n",
    "val_data['sentences'] = val_data['article'].apply(sent_tokenize)\n",
    "test_data['sentences'] = test_data['article'].apply(sent_tokenize)\n",
    "\n",
    "# Save data after sentence tokenization\n",
    "train_data.to_csv('tokenized_train_data.csv', index=False)\n",
    "val_data.to_csv('tokenized_val_data.csv', index=False)\n",
    "test_data.to_csv('tokenized_test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7862220-f2cc-47d8-9e41-09722831bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d784b8a7-9806-4b40-b821-348ef7f2a01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65bd17dd6a94cd1ba80e5f4cd96992c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45aa86433d55400288a55fab409e2c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614fb0719c164d4f8e3d032611db3c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['article'], max_length=512, truncation=True)\n",
    "    labels = tokenizer(examples['highlights'], max_length=150, truncation=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc82935f-e5fe-404f-8523-ad0447ede2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the tokenized datasets\n",
    "tokenized_train_dataset = load_from_disk('tokenized_datasets/train')\n",
    "tokenized_val_dataset = load_from_disk('tokenized_datasets/val')\n",
    "tokenized_test_dataset = load_from_disk('tokenized_datasets/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d167e5d0-fad4-4b95-a758-0b7703474d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\alexi\\anaconda3\\lib\\site-packages (0.33.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from accelerate) (2.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from accelerate) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.6.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92fa9149-620b-4528-b7b6-e844ddeb7efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33.0\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab814086-2a89-43fc-8ae7-b3cb7a6586fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\alexi\\anaconda3\\lib\\site-packages (0.33.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\alexi\\anaconda3\\lib\\site-packages (4.42.3)\n",
      "Requirement already satisfied: torch in c:\\users\\alexi\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\alexi\\anaconda3\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from accelerate) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alexi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate transformers torch datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4d67ef-0c63-4ff2-9d41-6768a3d66d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the tokenized datasets\n",
    "tokenized_train_dataset = load_from_disk('tokenized_datasets/train')\n",
    "tokenized_val_dataset = load_from_disk('tokenized_datasets/val')\n",
    "tokenized_test_dataset = load_from_disk('tokenized_datasets/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96bc739d-3fed-4698-8ddc-35d41e3c7899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958a82a3f2814ea68794eb876129dee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd5c96d7ff04b60800086749599bf0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8cda9bc7ea42e7834b4739c8c4d1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "\n",
    "# Function to adjust padding\n",
    "def adjust_padding(examples, max_length=512):\n",
    "    # Adjust inputs\n",
    "    inputs = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"input_ids\"], \"attention_mask\": examples[\"attention_mask\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Adjust labels\n",
    "    labels = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"labels\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Convert tensors to lists\n",
    "    examples[\"input_ids\"] = inputs[\"input_ids\"].tolist()\n",
    "    examples[\"attention_mask\"] = inputs[\"attention_mask\"].tolist()\n",
    "    examples[\"labels\"] = labels[\"input_ids\"].tolist()\n",
    "    return examples\n",
    "\n",
    "# Adjust padding for each dataset\n",
    "max_length_article = 512\n",
    "max_length_summary = 150\n",
    "\n",
    "tokenized_train_dataset = tokenized_train_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
    "tokenized_val_dataset = tokenized_val_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
    "tokenized_test_dataset = tokenized_test_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b20329a-0a8d-460b-871c-b6902ccecca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and selecting smaller train dataset...\n",
      "Shuffling and selecting smaller val dataset...\n",
      "Adjusting padding for smaller train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03490827aa04789894c98a7843d62f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting padding for smaller val dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3978fed18404009adedcfeade156cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "\n",
    "# Load the tokenized datasets\n",
    "tokenized_train_dataset = load_from_disk('tokenized_datasets/train')\n",
    "tokenized_val_dataset = load_from_disk('tokenized_datasets/val')\n",
    "tokenized_test_dataset = load_from_disk('tokenized_datasets/test')\n",
    "\n",
    "# Function to shuffle and select a subset of the dataset with progress bar\n",
    "def shuffle_and_select(dataset, num_samples, seed=42):\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.seed(seed)\n",
    "    random.shuffle(indices)\n",
    "    selected_indices = indices[:num_samples]\n",
    "    subset = dataset.select(selected_indices)\n",
    "    return subset\n",
    "\n",
    "# Select a smaller subset of the dataset with progress bar\n",
    "print(\"Shuffling and selecting smaller train dataset...\")\n",
    "small_train_dataset = shuffle_and_select(tokenized_train_dataset, 5000)\n",
    "print(\"Shuffling and selecting smaller val dataset...\")\n",
    "small_val_dataset = shuffle_and_select(tokenized_val_dataset, 1000)\n",
    "\n",
    "# Adjust padding for smaller datasets\n",
    "def adjust_padding(examples, max_length=512):\n",
    "    # Adjust inputs\n",
    "    inputs = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"input_ids\"], \"attention_mask\": examples[\"attention_mask\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Adjust labels\n",
    "    labels = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"labels\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Convert tensors to lists\n",
    "    examples[\"input_ids\"] = inputs[\"input_ids\"].tolist()\n",
    "    examples[\"attention_mask\"] = inputs[\"attention_mask\"].tolist()\n",
    "    examples[\"labels\"] = labels[\"input_ids\"].tolist()\n",
    "    return examples\n",
    "\n",
    "print(\"Adjusting padding for smaller train dataset...\")\n",
    "small_train_dataset = small_train_dataset.map(lambda examples: adjust_padding(examples, max_length=512), batched=True)\n",
    "print(\"Adjusting padding for smaller val dataset...\")\n",
    "small_val_dataset = small_val_dataset.map(lambda examples: adjust_padding(examples, max_length=512), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b332fd32-44e7-4a1f-ae11-3d0b3afaade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['article'], max_length=512, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(examples['highlights'], max_length=150, truncation=True, padding='max_length')\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86832477-b5a7-41f9-bc83-3903d0a810f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "\n",
    "# Load the tokenized datasets\n",
    "tokenized_train_dataset = load_from_disk('tokenized_datasets/train')\n",
    "tokenized_val_dataset = load_from_disk('tokenized_datasets/val')\n",
    "tokenized_test_dataset = load_from_disk('tokenized_datasets/test')\n",
    "\n",
    "# Function to adjust padding\n",
    "def adjust_padding(examples, max_length=512):\n",
    "    # Adjust inputs\n",
    "    inputs = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"input_ids\"], \"attention_mask\": examples[\"attention_mask\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Adjust labels\n",
    "    labels = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"labels\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Convert tensors to lists\n",
    "    examples[\"input_ids\"] = inputs[\"input_ids\"].tolist()\n",
    "    examples[\"attention_mask\"] = inputs[\"attention_mask\"].tolist()\n",
    "    examples[\"labels\"] = labels[\"input_ids\"].tolist()\n",
    "    return examples\n",
    "\n",
    "# Adjust padding for each dataset\n",
    "max_length_article = 512\n",
    "max_length_summary = 150\n",
    "\n",
    "tokenized_train_dataset = tokenized_train_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
    "tokenized_val_dataset = tokenized_val_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
    "tokenized_test_dataset = tokenized_test_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cba247e-425f-48c0-9775-a7744381c45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 5:12:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.321402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('saved_model/t5-small\\\\tokenizer_config.json',\n",
       " 'saved_model/t5-small\\\\special_tokens_map.json',\n",
       " 'saved_model/t5-small\\\\spiece.model',\n",
       " 'saved_model/t5-small\\\\added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# Load the model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Adjust training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,  # Reduce the number of epochs\n",
    "    per_device_train_batch_size=8,  # Increase the batch size if you have enough GPU memory\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=True,  # Use mixed precision training if supported by your hardware\n",
    "    disable_tqdm=False,  # Ensure tqdm progress bar is enabled\n",
    "    report_to=\"none\"  # Ensure no integration with external logging services\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_val_dataset,\n",
    ")\n",
    "\n",
    "# Show progress with tqdm\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('saved_model/t5-small')\n",
    "tokenizer.save_pretrained('saved_model/t5-small')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71e91a-04b5-43fc-9344-dcd69232c414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
