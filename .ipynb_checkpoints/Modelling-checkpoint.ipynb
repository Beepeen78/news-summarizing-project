{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6c8240d-fbab-4a56-8e98-188e02f6d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_from_disk\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70824067",
   "metadata": {},
   "source": [
    "Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d784b8a7-9806-4b40-b821-348ef7f2a01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65bd17dd6a94cd1ba80e5f4cd96992c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45aa86433d55400288a55fab409e2c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614fb0719c164d4f8e3d032611db3c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pre-trained T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Define a function to tokenize the 'article' and 'highlights' columns\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the 'article' text with a maximum length of 512 tokens, truncating longer sequences\n",
    "    model_inputs = tokenizer(examples['article'], max_length=512, truncation=True)\n",
    "    \n",
    "    # Tokenize the 'highlights' text (used as labels) with a maximum length of 150 tokens\n",
    "    labels = tokenizer(examples['highlights'], max_length=150, truncation=True)\n",
    "    \n",
    "    # Set the 'input_ids' from the tokenized highlights as labels for the model\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply the tokenization function to the training, validation, and test datasets\n",
    "# The map function applies the tokenization in batches for efficiency\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d167e5d0-fad4-4b95-a758-0b7703474d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92fa9149-620b-4528-b7b6-e844ddeb7efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33.0\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab814086-2a89-43fc-8ae7-b3cb7a6586fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate transformers torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4d67ef-0c63-4ff2-9d41-6768a3d66d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the pre-tokenized datasets from disk\n",
    "# The datasets are stored in a specified directory and are loaded into Dataset objects\n",
    "tokenized_train_dataset = load_from_disk('tokenized_datasets/train')\n",
    "tokenized_val_dataset = load_from_disk('tokenized_datasets/val')\n",
    "tokenized_test_dataset = load_from_disk('tokenized_datasets/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3e90f4",
   "metadata": {},
   "source": [
    "Adjusting Padding for Tokenized Text Data in T5 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96bc739d-3fed-4698-8ddc-35d41e3c7899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958a82a3f2814ea68794eb876129dee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd5c96d7ff04b60800086749599bf0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8cda9bc7ea42e7834b4739c8c4d1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "\n",
    "# Function to adjust padding\n",
    "def adjust_padding(examples, max_length=512):\n",
    "    # Adjust inputs\n",
    "    inputs = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"input_ids\"], \"attention_mask\": examples[\"attention_mask\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Adjust labels\n",
    "    labels = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"labels\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Convert tensors to lists\n",
    "    examples[\"input_ids\"] = inputs[\"input_ids\"].tolist()\n",
    "    examples[\"attention_mask\"] = inputs[\"attention_mask\"].tolist()\n",
    "    examples[\"labels\"] = labels[\"input_ids\"].tolist()\n",
    "    return examples\n",
    "\n",
    "# Adjust padding for each dataset\n",
    "max_length_article = 512\n",
    "max_length_summary = 150\n",
    "\n",
    "tokenized_train_dataset = tokenized_train_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
    "tokenized_val_dataset = tokenized_val_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
    "tokenized_test_dataset = tokenized_test_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1476431f",
   "metadata": {},
   "source": [
    "Shuffling, Subsampling, and Padding Adjustments for T5 Model Training on Smaller Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc406811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from datasets import load_from_disk\\nimport torch\\nfrom transformers import T5Tokenizer\\n\\n# Load the tokenizer\\ntokenizer = T5Tokenizer.from_pretrained(\\'t5-small\\', legacy=False)\\n\\n# Load the tokenized datasets\\ntokenized_train_dataset = load_from_disk(\\'tokenized_datasets/train\\')\\ntokenized_val_dataset = load_from_disk(\\'tokenized_datasets/val\\')\\ntokenized_test_dataset = load_from_disk(\\'tokenized_datasets/test\\')\\n\\n# Function to adjust padding\\ndef adjust_padding(examples, max_length=512):\\n    # Adjust inputs\\n    inputs = tokenizer.pad(\\n        {\"input_ids\": examples[\"input_ids\"], \"attention_mask\": examples[\"attention_mask\"]},\\n        padding=\"max_length\",\\n        max_length=max_length,\\n        return_tensors=\"pt\"\\n    )\\n    # Adjust labels\\n    labels = tokenizer.pad(\\n        {\"input_ids\": examples[\"labels\"]},\\n        padding=\"max_length\",\\n        max_length=max_length,\\n        return_tensors=\"pt\"\\n    )\\n    # Convert tensors to lists\\n    examples[\"input_ids\"] = inputs[\"input_ids\"].tolist()\\n    examples[\"attention_mask\"] = inputs[\"attention_mask\"].tolist()\\n    examples[\"labels\"] = labels[\"input_ids\"].tolist()\\n    return examples\\n\\n# Adjust padding for each dataset\\nmax_length_article = 512\\nmax_length_summary = 150\\n\\ntokenized_train_dataset = tokenized_train_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\\ntokenized_val_dataset = tokenized_val_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\\ntokenized_test_dataset = tokenized_test_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subsampling the dataset before adjusting the padding because this approach took too long as the data is huge\n",
    "'''from datasets import load_from_disk\n",
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "\n",
    "# Load the tokenized datasets\n",
    "tokenized_train_dataset = load_from_disk('tokenized_datasets/train')\n",
    "tokenized_val_dataset = load_from_disk('tokenized_datasets/val')\n",
    "tokenized_test_dataset = load_from_disk('tokenized_datasets/test')\n",
    "\n",
    "# Function to adjust padding\n",
    "def adjust_padding(examples, max_length=512):\n",
    "    # Adjust inputs\n",
    "    inputs = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"input_ids\"], \"attention_mask\": examples[\"attention_mask\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Adjust labels\n",
    "    labels = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"labels\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Convert tensors to lists\n",
    "    examples[\"input_ids\"] = inputs[\"input_ids\"].tolist()\n",
    "    examples[\"attention_mask\"] = inputs[\"attention_mask\"].tolist()\n",
    "    examples[\"labels\"] = labels[\"input_ids\"].tolist()\n",
    "    return examples\n",
    "\n",
    "# Adjust padding for each dataset\n",
    "max_length_article = 512\n",
    "max_length_summary = 150\n",
    "\n",
    "tokenized_train_dataset = tokenized_train_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
    "tokenized_val_dataset = tokenized_val_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
    "tokenized_test_dataset = tokenized_test_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b20329a-0a8d-460b-871c-b6902ccecca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and selecting smaller train dataset...\n",
      "Shuffling and selecting smaller val dataset...\n",
      "Adjusting padding for smaller train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03490827aa04789894c98a7843d62f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting padding for smaller val dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3978fed18404009adedcfeade156cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "\n",
    "# Load the tokenized datasets\n",
    "tokenized_train_dataset = load_from_disk('tokenized_datasets/train')\n",
    "tokenized_val_dataset = load_from_disk('tokenized_datasets/val')\n",
    "tokenized_test_dataset = load_from_disk('tokenized_datasets/test')\n",
    "\n",
    "# Function to shuffle and select a subset of the dataset with progress bar\n",
    "def shuffle_and_select(dataset, num_samples, seed=42):\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.seed(seed)\n",
    "    random.shuffle(indices)\n",
    "    selected_indices = indices[:num_samples]\n",
    "    subset = dataset.select(selected_indices)\n",
    "    return subset\n",
    "\n",
    "# Select a smaller subset of the dataset with progress bar\n",
    "print(\"Shuffling and selecting smaller train dataset...\")\n",
    "small_train_dataset = shuffle_and_select(tokenized_train_dataset, 5000)\n",
    "print(\"Shuffling and selecting smaller val dataset...\")\n",
    "small_val_dataset = shuffle_and_select(tokenized_val_dataset, 1000)\n",
    "\n",
    "# Adjust padding for smaller datasets\n",
    "def adjust_padding(examples, max_length=512):\n",
    "    # Adjust inputs\n",
    "    inputs = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"input_ids\"], \"attention_mask\": examples[\"attention_mask\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Adjust labels\n",
    "    labels = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"labels\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Convert tensors to lists\n",
    "    examples[\"input_ids\"] = inputs[\"input_ids\"].tolist()\n",
    "    examples[\"attention_mask\"] = inputs[\"attention_mask\"].tolist()\n",
    "    examples[\"labels\"] = labels[\"input_ids\"].tolist()\n",
    "    return examples\n",
    "\n",
    "print(\"Adjusting padding for smaller train dataset...\")\n",
    "small_train_dataset = small_train_dataset.map(lambda examples: adjust_padding(examples, max_length=512), batched=True)\n",
    "print(\"Adjusting padding for smaller val dataset...\")\n",
    "small_val_dataset = small_val_dataset.map(lambda examples: adjust_padding(examples, max_length=512), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b332fd32-44e7-4a1f-ae11-3d0b3afaade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the 'article' and 'highlights' columns from the dataset.\n",
    "# tokenize text data and prepare it for input into a T5 model\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['article'], max_length=512, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(examples['highlights'], max_length=150, truncation=True, padding='max_length')\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e225be3f",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cba247e-425f-48c0-9775-a7744381c45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 5:12:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.321402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('saved_model/t5-small\\\\tokenizer_config.json',\n",
       " 'saved_model/t5-small\\\\special_tokens_map.json',\n",
       " 'saved_model/t5-small\\\\spiece.model',\n",
       " 'saved_model/t5-small\\\\added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# Load the model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Adjust training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,  # Reduce the number of epochs\n",
    "    per_device_train_batch_size=8,  # Increase the batch size if you have enough GPU memory\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=True,  # Use mixed precision training if supported by your hardware\n",
    "    disable_tqdm=False,  # Ensure tqdm progress bar is enabled\n",
    "    report_to=\"none\"  # Ensure no integration with external logging services\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_val_dataset,\n",
    ")\n",
    "\n",
    "# Show progress with tqdm\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('saved_model/t5-small')\n",
    "tokenizer.save_pretrained('saved_model/t5-small')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71e91a-04b5-43fc-9344-dcd69232c414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
