{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLYMPDroqH3F"
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "import torch\n",
        "from transformers import T5Tokenizer\n",
        "!pip install accelerate -U\n",
        "import accelerate\n",
        "print(accelerate._version_)\n",
        "pip install accelerate transformers torch datasets\n",
        "# Load the tokenized datasets\n",
        "tokenized_train_dataset = load_from_disk('tokenized_datasets/train')\n",
        "tokenized_val_dataset = load_from_disk('tokenized_datasets/val')\n",
        "tokenized_test_dataset = load_from_disk('tokenized_datasets/test')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
        "\n",
        "# Function to adjust padding\n",
        "def adjust_padding(examples, max_length=512):\n",
        "    # Adjust inputs\n",
        "    inputs = tokenizer.pad(\n",
        "        {\"input_ids\": examples[\"input_ids\"], \"attention_mask\": examples[\"attention_mask\"]},\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    # Adjust labels\n",
        "    labels = tokenizer.pad(\n",
        "        {\"input_ids\": examples[\"labels\"]},\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    # Convert tensors to lists\n",
        "    examples[\"input_ids\"] = inputs[\"input_ids\"].tolist()\n",
        "    examples[\"attention_mask\"] = inputs[\"attention_mask\"].tolist()\n",
        "    examples[\"labels\"] = labels[\"input_ids\"].tolist()\n",
        "    return examples\n",
        "\n",
        "# Adjust padding for each dataset\n",
        "max_length_article = 512\n",
        "max_length_summary = 150\n",
        "\n",
        "tokenized_train_dataset = tokenized_train_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
        "tokenized_val_dataset = tokenized_val_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
        "tokenized_test_dataset = tokenized_test_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)"
      ],
      "metadata": {
        "id": "0Ffn5LGXqSZA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}