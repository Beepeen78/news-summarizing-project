{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17a85f53-994f-4d19-8c07-4b8a9498e41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         id  \\\n",
      "0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
      "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
      "2  00027e965c8264c35cc1bc55556db388da82b07f   \n",
      "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
      "4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n",
      "\n",
      "                                             article  \\\n",
      "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
      "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
      "2  A drunk driver who killed a young woman in a h...   \n",
      "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
      "4  Fleetwood are the only team still to have a 10...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Bishop John Folda, of North Dakota, is taking ...  \n",
      "1  Criminal complaint: Cop used his role to help ...  \n",
      "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
      "3  Nina dos Santos says Europe must be ready to a...  \n",
      "4  Fleetwood top of League One after 2-0 win at S...  \n",
      "                                         id  \\\n",
      "0  61df4979ac5fcc2b71be46ed6fe5a46ce7f071c3   \n",
      "1  21c0bd69b7e7df285c3d1b1cf56d4da925980a68   \n",
      "2  56f340189cd128194b2e7cb8c26bb900e3a848b4   \n",
      "3  00a665151b89a53e5a08a389df8334f4106494c2   \n",
      "4  9f6fbd3c497c4d28879bebebea220884f03eb41a   \n",
      "\n",
      "                                             article  \\\n",
      "0  Sally Forrest, an actress-dancer who graced th...   \n",
      "1  A middle-school teacher in China has inked hun...   \n",
      "2  A man convicted of killing the father and sist...   \n",
      "3  Avid rugby fan Prince Harry could barely watch...   \n",
      "4  A Triple M Radio producer has been inundated w...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Sally Forrest, an actress-dancer who graced th...  \n",
      "1  Works include pictures of Presidential Palace ...  \n",
      "2  Iftekhar Murtaza, 29, was convicted a year ago...  \n",
      "3  Prince Harry in attendance for England's crunc...  \n",
      "4  Nick Slater's colleagues uploaded a picture to...  \n",
      "                                         id  \\\n",
      "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
      "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
      "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
      "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
      "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
      "\n",
      "                                             article  \\\n",
      "0  Ever noticed how plane seats appear to be gett...   \n",
      "1  A drunk teenage boy had to be rescued by secur...   \n",
      "2  Dougie Freedman is on the verge of agreeing a ...   \n",
      "3  Liverpool target Neto is also wanted by PSG an...   \n",
      "4  Bruce Jenner will break his silence in a two-h...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Experts question if  packed out planes are put...  \n",
      "1  Drunk teenage boy climbed into lion enclosure ...  \n",
      "2  Nottingham Forest are close to extending Dougi...  \n",
      "3  Fiorentina goalkeeper Neto has been linked wit...  \n",
      "4  Tell-all interview with the reality TV star, 6...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset = pd.read_csv('train.csv')\n",
    "val_dataset = pd.read_csv('validation.csv')\n",
    "test_dataset = pd.read_csv('test.csv')\n",
    "\n",
    "# Display the first few rows of the datasets to understand their structure\n",
    "print(train_dataset.head())\n",
    "print(val_dataset.head())\n",
    "print(test_dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85f1ae21-5174-4f5b-97ed-a63da9809db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's clean the article and highlights columns by removing unnecessary characters, converting to lowercase, etc.\n",
    "import re\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove text in square brackets\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'<.*?>+', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Apply cleaning on the article and highlights columns\n",
    "train_dataset['article'] = train_dataset['article'].apply(clean_text)\n",
    "train_dataset['highlights'] = train_dataset['highlights'].apply(clean_text)\n",
    "\n",
    "val_dataset['article'] = val_dataset['article'].apply(clean_text)\n",
    "val_dataset['highlights'] = val_dataset['highlights'].apply(clean_text)\n",
    "\n",
    "test_dataset['article'] = test_dataset['article'].apply(clean_text)\n",
    "test_dataset['highlights'] = test_dataset['highlights'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92801873-0420-480e-9296-85702d7d8522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Next, we'll segment the sentences in the articles, although this step may be optional depending on the dataset and model requirements.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def segment_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "# Apply sentence segmentation on the dataset\n",
    "train_dataset['article'] = train_dataset['article'].apply(segment_sentences)\n",
    "val_dataset['article'] = val_dataset['article'].apply(segment_sentences)\n",
    "test_dataset['article'] = test_dataset['article'].apply(segment_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36b9ce70-a41a-420a-9b89-e310443b830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling outliars\n",
    "# Remove articles that are too short or too long\n",
    "def filter_outliers(text, min_len=50, max_len=1000):\n",
    "    if len(text.split()) < min_len or len(text.split()) > max_len:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Filter the datasets\n",
    "train_dataset = train_dataset[train_dataset['article'].apply(filter_outliers)]\n",
    "val_dataset = val_dataset[val_dataset['article'].apply(filter_outliers)]\n",
    "test_dataset = test_dataset[test_dataset['article'].apply(filter_outliers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d2bd1f5-503c-4799-98c5-a09a4418ea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the tokenized datasets\n",
    "train_dataset = load_from_disk('data/train_tokenized')\n",
    "val_dataset = load_from_disk('data/val_tokenized')\n",
    "test_dataset = load_from_disk('data/test_tokenized')\n",
    "\n",
    "print(\"Tokenized datasets loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db432e8f-fe86-4c3f-b080-316e5f74545f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4e3e39aec94b09b7c2d379580ac41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/241457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexi\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a447741b764e22a5450ffd882db890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ea65fc5cbf425288bc19d47c7babd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9567 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, Trainer, TrainingArguments, PegasusTokenizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
    "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')\n",
    "\n",
    "def tokenize_data(example):\n",
    "    inputs = tokenizer(\n",
    "        example['article'],\n",
    "        padding='max_length',  # Pad to the maximum length\n",
    "        truncation=True,       # Truncate sequences to the max length\n",
    "        max_length=512         # Set the maximum length (adjust as needed)\n",
    "    )\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            example['highlights'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128       # Adjust as needed for the target summary length\n",
    "        )\n",
    "    \n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs\n",
    "\n",
    "# Apply tokenization to the datasets\n",
    "train_dataset = train_dataset.map(tokenize_data, batched=True, batch_size=32)\n",
    "val_dataset = val_dataset.map(tokenize_data, batched=True, batch_size=32)\n",
    "test_dataset = test_dataset.map(tokenize_data, batched=True, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "714e0b15-0f04-4d4b-b726-c45754ce0a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the processed datasets\n",
    "from datasets import load_from_disk\n",
    "train_dataset = load_from_disk('./final_datasets/train_dataset')\n",
    "val_dataset = load_from_disk('./final_datasets/val_dataset')\n",
    "test_dataset = load_from_disk('./final_datasets/test_dataset')\n",
    "\n",
    "print(\"Datasets loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "831eb1d2-c65a-47bf-bdc8-c4f680863cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69233b75c3c4c64aea21b922f90a93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/241457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882179c5b34a4ec998cd53e3f31297eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6aa35e5e19a4096b3c08c9e207204b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9567 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize_data, batched=True, batch_size=32)\n",
    "val_dataset = val_dataset.map(tokenize_data, batched=True, batch_size=32)\n",
    "test_dataset = test_dataset.map(tokenize_data, batched=True, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7548f118-b9f4-44e8-a538-27351d53d3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'article', 'highlights', '__index_level_0__', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9df248c5-544b-4747-8ebe-ecddbed4cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd928ec6-1492-48ad-a6d6-6bb92290f6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\alexi\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='30184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    2/30184 : < :, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, Trainer, TrainingArguments, PegasusTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
    "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,  # Updated batch size\n",
    "    per_device_eval_batch_size=16,   # Updated batch size\n",
    "    num_train_epochs=2,              # Updated number of epochs\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized!\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./results/pegasus-summarizer')\n",
    "tokenizer.save_pretrained('./results/pegasus-summarizer')\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "eval_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Test Loss: {eval_results['eval_loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "895678b1-f2ff-40ed-a035-140d5e3d7a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 700 examples from each dataset\n",
    "train_sample = train_dataset.shuffle(seed=42).select(range(700))\n",
    "val_sample = val_dataset.shuffle(seed=42).select(range(700))\n",
    "test_sample = test_dataset.shuffle(seed=42).select(range(700))\n",
    "\n",
    "# Now use these samples for training and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "368aff28-e9c6-4acb-92ac-106bc34845a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3fabd2124584b93a415795c883e0194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/241457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Apply the preprocessing to the datasets\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mmap(preprocess_data, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m     15\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m val_dataset\u001b[38;5;241m.\u001b[39mmap(preprocess_data, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m     16\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mmap(preprocess_data, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    565\u001b[0m }\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3161\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3156\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3157\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3158\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3159\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3160\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3161\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3162\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3163\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3552\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3548\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   3549\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[0;32m   3550\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3551\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3552\u001b[0m     batch \u001b[38;5;241m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[0;32m   3553\u001b[0m         batch,\n\u001b[0;32m   3554\u001b[0m         indices,\n\u001b[0;32m   3555\u001b[0m         check_same_num_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(shard\u001b[38;5;241m.\u001b[39mlist_indexes()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   3556\u001b[0m         offset\u001b[38;5;241m=\u001b[39moffset,\n\u001b[0;32m   3557\u001b[0m     )\n\u001b[0;32m   3558\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3561\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3421\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   3420\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 3421\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3423\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3424\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[0;32m   3425\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(example)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_data\u001b[39m(example):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Tokenize the input text (article) and the target text (highlights)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m], max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m     labels \u001b[38;5;241m=\u001b[39m tokenizer(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighlights\u001b[39m\u001b[38;5;124m'\u001b[39m], max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Set the labels (decoder input ids)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "def preprocess_data(example):\n",
    "    # Tokenize the input text (article) and the target text (highlights)\n",
    "    inputs = tokenizer(example['article'], max_length=512, padding='max_length', truncation=True)\n",
    "    labels = tokenizer(example['highlights'], max_length=128, padding='max_length', truncation=True)\n",
    "\n",
    "    # Set the labels (decoder input ids)\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# Apply the preprocessing to the datasets\n",
    "train_dataset = train_dataset.map(preprocess_data, batched=True, batch_size=32)\n",
    "val_dataset = val_dataset.map(preprocess_data, batched=True, batch_size=32)\n",
    "test_dataset = test_dataset.map(preprocess_data, batched=True, batch_size=32)\n",
    "\n",
    "# Save the preprocessed datasets\n",
    "train_dataset.save_to_disk('preprocessed_train_dataset')\n",
    "val_dataset.save_to_disk('preprocessed_val_dataset')\n",
    "test_dataset.save_to_disk('preprocessed_test_dataset')\n",
    "\n",
    "print(\"Datasets have been tokenized, preprocessed, and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a77bc4-c8d7-4dd4-8e8b-0fbd08f877fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the preprocessed datasets\n",
    "train_dataset = load_from_disk('preprocessed_train_dataset')\n",
    "val_dataset = load_from_disk('preprocessed_val_dataset')\n",
    "test_dataset = load_from_disk('preprocessed_test_dataset')\n",
    "\n",
    "# Reduce the size of the datasets to 1000 samples\n",
    "train_sample = train_dataset.select(range(1000))\n",
    "val_sample = val_dataset.select(range(100))\n",
    "test_sample = test_dataset.select(range(100))\n",
    "\n",
    "# Now you can use these smaller datasets for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d322140-3521-4380-aa88-5b36427d5c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\alexi\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 12:21:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.979800</td>\n",
       "      <td>6.770640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.594800</td>\n",
       "      <td>6.676665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'num_beams': 8, 'length_penalty': 0.6, 'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'num_beams': 8, 'length_penalty': 0.6, 'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'num_beams': 8, 'length_penalty': 0.6, 'forced_eos_token_id': 1}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'num_beams': 8, 'length_penalty': 0.6, 'forced_eos_token_id': 1}\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# Load the Pegasus model\n",
    "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample,\n",
    "    eval_dataset=val_sample,\n",
    ")\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./results/pegasus-small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c4b544d-7e3e-42b3-9607-141527c9f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# Specify the model path if you saved the model earlier\n",
    "model_path = './results/pegasus-small'\n",
    "\n",
    "# Reload the tokenizer and model\n",
    "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d3f127d-c09f-4ff6-b6b8-21b10c39a4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'num_beams': 8, 'length_penalty': 0.6, 'forced_eos_token_id': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to save the model and tokenizer\n",
    "save_directory = './results/pegasus-small'\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "print(\"Model and tokenizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0e4e79f-f483-44f0-8d21-81597bffe26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text tokenized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Example text to summarize\n",
    "text_to_summarize = \"\"\"\n",
    "Your input text goes here. This should be a long paragraph or a couple of paragraphs that you want to summarize.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text_to_summarize, max_length=512, return_tensors='pt', truncation=True)\n",
    "\n",
    "print(\"Input text tokenized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66f2b7d2-1fc2-47aa-97db-aac0a0ecd905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary generated successfully!\n",
      "\n",
      "Summary:\n",
      " Do you have a short paragraph or two of paragraphs you want to include in this story?\n"
     ]
    }
   ],
   "source": [
    "# Generate the summary\n",
    "summary_ids = model.generate(\n",
    "    inputs['input_ids'], \n",
    "    max_length=64, \n",
    "    num_beams=8, \n",
    "    length_penalty=0.6, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode the generated summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Summary generated successfully!\")\n",
    "print(\"\\nSummary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca5f36b0-7d71-4259-a796-38626176e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(\n",
    "    inputs['input_ids'], \n",
    "    max_length=128,  # Increase max length for more detailed summaries\n",
    "    num_beams=5,     # Experiment with different numbers of beams\n",
    "    length_penalty=1.0,  # Adjust length penalty to control summary length\n",
    "    early_stopping=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a20fefcf-ecae-49a9-aec1-01de43fcec3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_path = './results/pegasus-small'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_path)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "def summarize(text):\n",
    "    inputs = tokenizer(text, max_length=1024, return_tensors='pt', truncation=True)\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=150,\n",
    "        num_beams=10,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=summarize,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Enhanced Pegasus Text Summarizer\",\n",
    "    description=\"Enter text to generate a summary using the fine-tuned Pegasus model with improved parameters.\"\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "432e48cf-6ac9-4896-833b-7ab01968f758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7875\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import gradio as gr\n",
    "import re\n",
    "\n",
    "# Load Pegasus model and tokenizer\n",
    "pegasus_model_path = './results/pegasus-small'\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_model_path)\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_model_path)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove extra spaces, newlines, and tabs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def generate_pegasus_summary(text):\n",
    "    try:\n",
    "        # Preprocess the text\n",
    "        text = preprocess_text(text)\n",
    "\n",
    "        # Add a lead-in prompt to the text\n",
    "        prompt = \"Summarize the following: \"\n",
    "        text = prompt + text\n",
    "\n",
    "        # Tokenize the input text\n",
    "        inputs = pegasus_tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "        # Generate the summary\n",
    "        summary_ids = pegasus_model.generate(\n",
    "            **inputs,\n",
    "            max_length=150,  # Adjust this if necessary\n",
    "            min_length=30,   # Set a minimum length to encourage longer summaries\n",
    "            num_beams=6,    # Reduce the beams to encourage diversity\n",
    "            length_penalty=1.0,  # You can adjust this\n",
    "            no_repeat_ngram_size=3,  # Prevents repetition of 3-grams\n",
    "            early_stopping=False,  # Allow the model to continue until it generates a sufficient summary\n",
    "            temperature=0.9,  # Increase temperature for more randomness\n",
    "            top_k=50,  # Consider adding top_k for better diversity\n",
    "            top_p=0.9,  # Consider adding top_p for better diversity\n",
    "        )\n",
    "\n",
    "        # Decode the summary\n",
    "        summary = pegasus_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Post-process to remove the first line if it closely matches the input\n",
    "        input_first_sentence = text.split('.')[0]\n",
    "        if summary.startswith(input_first_sentence):\n",
    "            summary = summary[len(input_first_sentence):].strip()\n",
    "\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Interface using Gradio\n",
    "iface = gr.Interface(\n",
    "    fn=generate_pegasus_summary, \n",
    "    inputs=gr.Textbox(lines=10, label=\"Input Text\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Dynamic Pegasus Summarizer\",\n",
    "    description=\"Generate a summary using the fine-tuned Pegasus model with improved dynamicity.\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
