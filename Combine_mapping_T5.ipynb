{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a68c823",
   "metadata": {},
   "source": [
    "Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6c8240d-fbab-4a56-8e98-188e02f6d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_from_disk\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592da1db",
   "metadata": {},
   "source": [
    "Loading the dataset & data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a024b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('validation.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41ec61cc-93d6-4bac-8e72-07a5f629ad24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         id  \\\n",
      "0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
      "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
      "2  00027e965c8264c35cc1bc55556db388da82b07f   \n",
      "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
      "4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n",
      "\n",
      "                                             article  \\\n",
      "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
      "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
      "2  A drunk driver who killed a young woman in a h...   \n",
      "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
      "4  Fleetwood are the only team still to have a 10...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Bishop John Folda, of North Dakota, is taking ...  \n",
      "1  Criminal complaint: Cop used his role to help ...  \n",
      "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
      "3  Nina dos Santos says Europe must be ready to a...  \n",
      "4  Fleetwood top of League One after 2-0 win at S...  \n",
      "                                         id  \\\n",
      "0  61df4979ac5fcc2b71be46ed6fe5a46ce7f071c3   \n",
      "1  21c0bd69b7e7df285c3d1b1cf56d4da925980a68   \n",
      "2  56f340189cd128194b2e7cb8c26bb900e3a848b4   \n",
      "3  00a665151b89a53e5a08a389df8334f4106494c2   \n",
      "4  9f6fbd3c497c4d28879bebebea220884f03eb41a   \n",
      "\n",
      "                                             article  \\\n",
      "0  Sally Forrest, an actress-dancer who graced th...   \n",
      "1  A middle-school teacher in China has inked hun...   \n",
      "2  A man convicted of killing the father and sist...   \n",
      "3  Avid rugby fan Prince Harry could barely watch...   \n",
      "4  A Triple M Radio producer has been inundated w...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Sally Forrest, an actress-dancer who graced th...  \n",
      "1  Works include pictures of Presidential Palace ...  \n",
      "2  Iftekhar Murtaza, 29, was convicted a year ago...  \n",
      "3  Prince Harry in attendance for England's crunc...  \n",
      "4  Nick Slater's colleagues uploaded a picture to...  \n",
      "                                         id  \\\n",
      "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
      "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
      "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
      "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
      "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
      "\n",
      "                                             article  \\\n",
      "0  Ever noticed how plane seats appear to be gett...   \n",
      "1  A drunk teenage boy had to be rescued by secur...   \n",
      "2  Dougie Freedman is on the verge of agreeing a ...   \n",
      "3  Liverpool target Neto is also wanted by PSG an...   \n",
      "4  Bruce Jenner will break his silence in a two-h...   \n",
      "\n",
      "                                          highlights  \n",
      "0  Experts question if  packed out planes are put...  \n",
      "1  Drunk teenage boy climbed into lion enclosure ...  \n",
      "2  Nottingham Forest are close to extending Dougi...  \n",
      "3  Fiorentina goalkeeper Neto has been linked wit...  \n",
      "4  Tell-all interview with the reality TV star, 6...  \n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())\n",
    "print(val_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28eaa06",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76967017-c4cc-4f9f-992c-578527fd25ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing all letters to lowercase\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "train_data['article'] = train_data['article'].apply(preprocess)\n",
    "train_data['highlights'] = train_data['highlights'].apply(preprocess)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(preprocess)\n",
    "val_data['highlights'] = val_data['highlights'].apply(preprocess)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(preprocess)\n",
    "test_data['highlights'] = test_data['highlights'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a95d2-2009-4c61-8961-84edecf949b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to Hugging Face Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09ee66-310a-422b-8fac-0381a78f50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned datasets as CSV files without including the index\n",
    "train_data.to_csv('cleaned_train_data.csv', index=False)\n",
    "val_data.to_csv('cleaned_val_data.csv', index=False)\n",
    "test_data.to_csv('cleaned_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472fbe6-bcab-4df6-ae5e-04918aac472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK resources: stopwords list and punkt tokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Create a set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to remove stopwords from a given text\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)  # Tokenize the text into words\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]  # Filter out stopwords\n",
    "    return ' '.join(filtered_text)  # Join the words back into a single string\n",
    "\n",
    "# Apply the remove_stopwords function to the 'article' and 'highlights' columns of the datasets\n",
    "train_data['article'] = train_data['article'].apply(remove_stopwords)\n",
    "train_data['highlights'] = train_data['highlights'].apply(remove_stopwords)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(remove_stopwords)\n",
    "val_data['highlights'] = val_data['highlights'].apply(remove_stopwords)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(remove_stopwords)\n",
    "test_data['highlights'] = test_data['highlights'].apply(remove_stopwords)\n",
    "\n",
    "# Save the datasets after removing stopwords\n",
    "train_data.to_csv('stopwords_removed_train_data.csv', index=False)\n",
    "val_data.to_csv('stopwords_removed_val_data.csv', index=False)\n",
    "test_data.to_csv('stopwords_removed_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7572f-584f-46e2-9444-213909e457ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use SpaCy for lemmatization to convert words to their base forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b4c37b2-8247-4c47-a030-9b20f7ef8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c9e9eb-6986-4be5-a487-089c9c91a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy tqdm\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48cd0ac3-f788-4657-9771-3bc72a822942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 287113/287113 [3:35:38<00:00, 22.19it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 287113/287113 [14:04<00:00, 339.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 13368/13368 [08:01<00:00, 27.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 13368/13368 [00:41<00:00, 318.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 11490/11490 [07:00<00:00, 27.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 11490/11490 [00:35<00:00, 326.70it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('cleaned_train_data.csv')\n",
    "val_data = pd.read_csv('cleaned_val_data.csv')\n",
    "test_data = pd.read_csv('cleaned_test_data.csv')\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to all datasets\n",
    "train_data['article'] = train_data['article'].apply(clean_text)\n",
    "train_data['highlights'] = train_data['highlights'].apply(clean_text)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(clean_text)\n",
    "val_data['highlights'] = val_data['highlights'].apply(clean_text)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(clean_text)\n",
    "test_data['highlights'] = test_data['highlights'].apply(clean_text)\n",
    "\n",
    "# Load the pre-trained SpaCy model globally\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function for lemmatization using nlp.pipe for batch processing\n",
    "def lemmatize_texts_with_progress(texts):\n",
    "    lemmatized_texts = []\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=50, disable=['parser', 'ner']), total=len(texts)):\n",
    "        lemmatized_texts.append(' '.join([token.lemma_ for token in doc]))\n",
    "    return lemmatized_texts\n",
    "\n",
    "# Apply lemmatization with progress bar\n",
    "train_data['article'] = lemmatize_texts_with_progress(train_data['article'].tolist())\n",
    "train_data['highlights'] = lemmatize_texts_with_progress(train_data['highlights'].tolist())\n",
    "\n",
    "val_data['article'] = lemmatize_texts_with_progress(val_data['article'].tolist())\n",
    "val_data['highlights'] = lemmatize_texts_with_progress(val_data['highlights'].tolist())\n",
    "\n",
    "test_data['article'] = lemmatize_texts_with_progress(test_data['article'].tolist())\n",
    "test_data['highlights'] = lemmatize_texts_with_progress(test_data['highlights'].tolist())\n",
    "\n",
    "# Save lemmatized data\n",
    "train_data.to_csv('lemmatized_train_data.csv', index=False)\n",
    "val_data.to_csv('lemmatized_val_data.csv', index=False)\n",
    "test_data.to_csv('lemmatized_test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63fecb34-a6f3-4df0-8b0a-361bb08c8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Function to replace all numbers in the text with a placeholder <NUM>\n",
    "def replace_numbers(text):\n",
    "    return re.sub(r'\\d+', '<NUM>', text)\n",
    "\n",
    "# Function to replace rare words in the text with a placeholder <RARE>\n",
    "# A word is considered rare if its frequency is below the specified threshold (default is 5)\n",
    "def remove_rare_words(text, freq_threshold=5):\n",
    "    words = text.split()  # Split the text into words\n",
    "    word_freq = Counter(words)  # Count the frequency of each word\n",
    "    # Identify words that occur less than the frequency threshold\n",
    "    rare_words = {word for word, freq in word_freq.items() if freq < freq_threshold}\n",
    "    # Replace rare words with <RARE>, keep other words unchanged\n",
    "    filtered_text = [word if word not in rare_words else '<RARE>' for word in words]\n",
    "    return ' '.join(filtered_text)  # Join the words back into a single string\n",
    "\n",
    "# Apply number replacement and rare word removal to the 'article' and 'highlights' columns of all datasets\n",
    "train_data['article'] = train_data['article'].apply(replace_numbers).apply(remove_rare_words)\n",
    "train_data['highlights'] = train_data['highlights'].apply(replace_numbers).apply(remove_rare_words)\n",
    "\n",
    "val_data['article'] = val_data['article'].apply(replace_numbers).apply(remove_rare_words)\n",
    "val_data['highlights'] = val_data['highlights'].apply(replace_numbers).apply(remove_rare_words)\n",
    "\n",
    "test_data['article'] = test_data['article'].apply(replace_numbers).apply(remove_rare_words)\n",
    "test_data['highlights'] = test_data['highlights'].apply(replace_numbers).apply(remove_rare_words)\n",
    "\n",
    "# Save the processed datasets with handled numbers and rare words\n",
    "train_data.to_csv('handled_train_data.csv', index=False)  # Save the training data\n",
    "val_data.to_csv('handled_val_data.csv', index=False)      # Save the validation data\n",
    "test_data.to_csv('handled_test_data.csv', index=False)    # Save the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bbd06f1-63c3-4a2b-a698-c0a26066fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentence tokenization to the 'article' column\n",
    "# This splits the text of each article into a list of sentences\n",
    "train_data['sentences'] = train_data['article'].apply(sent_tokenize)\n",
    "val_data['sentences'] = val_data['article'].apply(sent_tokenize)\n",
    "test_data['sentences'] = test_data['article'].apply(sent_tokenize)\n",
    "\n",
    "# Save the datasets with the new 'sentences' column to CSV files\n",
    "train_data.to_csv('tokenized_train_data.csv', index=False)  # Save the training data\n",
    "val_data.to_csv('tokenized_val_data.csv', index=False)      # Save the validation data\n",
    "test_data.to_csv('tokenized_test_data.csv', index=False)    # Save the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7862220-f2cc-47d8-9e41-09722831bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to Hugging Face Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce5f98",
   "metadata": {},
   "source": [
    "Preparing for Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d784b8a7-9806-4b40-b821-348ef7f2a01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65bd17dd6a94cd1ba80e5f4cd96992c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45aa86433d55400288a55fab409e2c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614fb0719c164d4f8e3d032611db3c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pre-trained T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Define a function to tokenize the 'article' and 'highlights' columns\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the 'article' text with a maximum length of 512 tokens, truncating longer sequences\n",
    "    model_inputs = tokenizer(examples['article'], max_length=512, truncation=True)\n",
    "    \n",
    "    # Tokenize the 'highlights' text (used as labels) with a maximum length of 150 tokens\n",
    "    labels = tokenizer(examples['highlights'], max_length=150, truncation=True)\n",
    "    \n",
    "    # Set the 'input_ids' from the tokenized highlights as labels for the model\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply the tokenization function to the training, validation, and test datasets\n",
    "# The map function applies the tokenization in batches for efficiency\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d167e5d0-fad4-4b95-a758-0b7703474d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92fa9149-620b-4528-b7b6-e844ddeb7efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33.0\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab814086-2a89-43fc-8ae7-b3cb7a6586fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate transformers torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4d67ef-0c63-4ff2-9d41-6768a3d66d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the pre-tokenized datasets from disk\n",
    "# The datasets are stored in a specified directory and are loaded into Dataset objects\n",
    "tokenized_train_dataset = load_from_disk('tokenized_datasets/train')\n",
    "tokenized_val_dataset = load_from_disk('tokenized_datasets/val')\n",
    "tokenized_test_dataset = load_from_disk('tokenized_datasets/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab80612",
   "metadata": {},
   "source": [
    "Adjusting Padding for Tokenized Text Data in T5 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96bc739d-3fed-4698-8ddc-35d41e3c7899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958a82a3f2814ea68794eb876129dee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd5c96d7ff04b60800086749599bf0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8cda9bc7ea42e7834b4739c8c4d1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "\n",
    "# Function to adjust padding\n",
    "def adjust_padding(examples, max_length=512):\n",
    "    # Adjust inputs\n",
    "    inputs = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"input_ids\"], \"attention_mask\": examples[\"attention_mask\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Adjust labels\n",
    "    labels = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"labels\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Convert tensors to lists\n",
    "    examples[\"input_ids\"] = inputs[\"input_ids\"].tolist()\n",
    "    examples[\"attention_mask\"] = inputs[\"attention_mask\"].tolist()\n",
    "    examples[\"labels\"] = labels[\"input_ids\"].tolist()\n",
    "    return examples\n",
    "\n",
    "# Adjust padding for each dataset\n",
    "max_length_article = 512\n",
    "max_length_summary = 150\n",
    "\n",
    "tokenized_train_dataset = tokenized_train_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
    "tokenized_val_dataset = tokenized_val_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
    "tokenized_test_dataset = tokenized_test_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd685159",
   "metadata": {},
   "source": [
    "Shuffling, Subsampling, and Padding Adjustments for T5 Model Training on Smaller Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf127e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsampling the dataset before adjusting the padding because the other approach took too long\n",
    "'''from datasets import load_from_disk\n",
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "\n",
    "# Load the tokenized datasets\n",
    "tokenized_train_dataset = load_from_disk('tokenized_datasets/train')\n",
    "tokenized_val_dataset = load_from_disk('tokenized_datasets/val')\n",
    "tokenized_test_dataset = load_from_disk('tokenized_datasets/test')\n",
    "\n",
    "# Function to adjust padding\n",
    "def adjust_padding(examples, max_length=512):\n",
    "    # Adjust inputs\n",
    "    inputs = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"input_ids\"], \"attention_mask\": examples[\"attention_mask\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Adjust labels\n",
    "    labels = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"labels\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Convert tensors to lists\n",
    "    examples[\"input_ids\"] = inputs[\"input_ids\"].tolist()\n",
    "    examples[\"attention_mask\"] = inputs[\"attention_mask\"].tolist()\n",
    "    examples[\"labels\"] = labels[\"input_ids\"].tolist()\n",
    "    return examples\n",
    "\n",
    "# Adjust padding for each dataset\n",
    "max_length_article = 512\n",
    "max_length_summary = 150\n",
    "\n",
    "tokenized_train_dataset = tokenized_train_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
    "tokenized_val_dataset = tokenized_val_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
    "tokenized_test_dataset = tokenized_test_dataset.map(lambda examples: adjust_padding(examples, max_length=max_length_article), batched=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b20329a-0a8d-460b-871c-b6902ccecca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and selecting smaller train dataset...\n",
      "Shuffling and selecting smaller val dataset...\n",
      "Adjusting padding for smaller train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03490827aa04789894c98a7843d62f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting padding for smaller val dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3978fed18404009adedcfeade156cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "\n",
    "# Load the tokenized datasets\n",
    "tokenized_train_dataset = load_from_disk('tokenized_datasets/train')\n",
    "tokenized_val_dataset = load_from_disk('tokenized_datasets/val')\n",
    "tokenized_test_dataset = load_from_disk('tokenized_datasets/test')\n",
    "\n",
    "# Function to shuffle and select a subset of the dataset with progress bar\n",
    "def shuffle_and_select(dataset, num_samples, seed=42):\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.seed(seed)\n",
    "    random.shuffle(indices)\n",
    "    selected_indices = indices[:num_samples]\n",
    "    subset = dataset.select(selected_indices)\n",
    "    return subset\n",
    "\n",
    "# Select a smaller subset of the dataset with progress bar\n",
    "print(\"Shuffling and selecting smaller train dataset...\")\n",
    "small_train_dataset = shuffle_and_select(tokenized_train_dataset, 5000)\n",
    "print(\"Shuffling and selecting smaller val dataset...\")\n",
    "small_val_dataset = shuffle_and_select(tokenized_val_dataset, 1000)\n",
    "\n",
    "# Adjust padding for smaller datasets\n",
    "def adjust_padding(examples, max_length=512):\n",
    "    # Adjust inputs\n",
    "    inputs = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"input_ids\"], \"attention_mask\": examples[\"attention_mask\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Adjust labels\n",
    "    labels = tokenizer.pad(\n",
    "        {\"input_ids\": examples[\"labels\"]},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Convert tensors to lists\n",
    "    examples[\"input_ids\"] = inputs[\"input_ids\"].tolist()\n",
    "    examples[\"attention_mask\"] = inputs[\"attention_mask\"].tolist()\n",
    "    examples[\"labels\"] = labels[\"input_ids\"].tolist()\n",
    "    return examples\n",
    "\n",
    "print(\"Adjusting padding for smaller train dataset...\")\n",
    "small_train_dataset = small_train_dataset.map(lambda examples: adjust_padding(examples, max_length=512), batched=True)\n",
    "print(\"Adjusting padding for smaller val dataset...\")\n",
    "small_val_dataset = small_val_dataset.map(lambda examples: adjust_padding(examples, max_length=512), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b332fd32-44e7-4a1f-ae11-3d0b3afaade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the 'article' and 'highlights' columns from the dataset.\n",
    "# tokenize text data and prepare it for input into a T5 model\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['article'], max_length=512, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(examples['highlights'], max_length=150, truncation=True, padding='max_length')\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ba0011",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cba247e-425f-48c0-9775-a7744381c45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 5:12:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.321402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('saved_model/t5-small\\\\tokenizer_config.json',\n",
       " 'saved_model/t5-small\\\\special_tokens_map.json',\n",
       " 'saved_model/t5-small\\\\spiece.model',\n",
       " 'saved_model/t5-small\\\\added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# Load the model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Adjust training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,  # Reduce the number of epochs\n",
    "    per_device_train_batch_size=8,  # Increase the batch size if you have enough GPU memory\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=True,  # Use mixed precision training if supported by your hardware\n",
    "    disable_tqdm=False,  # Ensure tqdm progress bar is enabled\n",
    "    report_to=\"none\"  # Ensure no integration with external logging services\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_val_dataset,\n",
    ")\n",
    "\n",
    "# Show progress with tqdm\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('saved_model/t5-small')\n",
    "tokenizer.save_pretrained('saved_model/t5-small')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71e91a-04b5-43fc-9344-dcd69232c414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
